{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18cc0eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# os.system('.env/Scripts/activate')\n",
    "# os.system('pip install datasets')\n",
    "import datasets\n",
    "from datasets import load_dataset\n",
    "raw_dataset = load_dataset('wmt/wmt19', \"zh-en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "433b3053",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Features, Value, DatasetDict\n",
    "self_dataset = load_dataset('csv', data_files='../Data/news_data.csv', features=Features({'abstract':Value('string')}))\n",
    "cut_raw_dataset = DatasetDict(\n",
    "    {\n",
    "        \"train\" : raw_dataset[\"train\"].select(range(210000)),\n",
    "        \"validation\" : raw_dataset[\"validation\"]\n",
    "    }\n",
    ")\n",
    "del raw_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fcf1c1ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Yujun_Doc\\Modeling\\.env\\lib\\site-packages\\transformers\\models\\marian\\tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'translation_text': 'The model uses a wordbox based on SentencePiece. SentencePiece is a popular word division algorithm that is particularly suitable for multi-language text processing.'}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "# os.system('pip install sentencepiece')\n",
    "split_datasets = cut_raw_dataset['train'].train_test_split(train_size=0.9, seed=20)\n",
    "translator = pipeline('translation', model='Helsinki-NLP/opus-mt-zh-en')\n",
    "translator(\"今天英伟达的股票激增10%！\")\n",
    "# print(split_datasets)\n",
    "# print(split_datasets['train'][1]['translation'])\n",
    "translator('模型使用的是基于 SentencePiece 的分词器。SentencePiece 是一种流行的分词算法，特别适合处理多语言文本。')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e34211",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Yujun_Doc\\Modeling\\.env\\lib\\site-packages\\transformers\\models\\marian\\tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('Helsinki-NLP/opus-mt-zh-en', return_tensors='pt') # pt means pytorch\n",
    "\n",
    "def\n",
    "    inputs = [i['zh'] for i in split_datasets['train']['translation']]  \n",
    "    targets = [i['en'] for i in split_datasets['train']['translation']]\n",
    "    model_inputs = tokenizer(inputs, text_target=targets, max_length=128, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ffa70e19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁他', '打了', '简单的', '比', '喻', ':', '我们', '都是', '有限', '而且', '脆弱的', '血', '肉', '之', '躯', ',', '究竟', '能', '把', '多少', '“', '东西', '▁', '”', '▁—', '▁—', '快', '餐', ',', '电视', '广告', ',', '豪', '华', '轿', '车', ',', '新', '奇', '玩', '艺', '和', '时', '装', '▁—', '▁—', '▁', '塞', '进', '自己的', '身体', '却', '又', '不致', '打', '乱', '心灵', '的', '平静', '呢', '?', '</s>']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'translation_text': 'He gave a simple metaphor: How many things do we have – fast foods, television advertisements, limo, new wonders and fashion – fit into our own bodies without disturbing the calm of mind?'}]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(tokenizer.convert_ids_to_tokens(model_inputs['input_ids'][1]))\n",
    "translator(inputs[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
